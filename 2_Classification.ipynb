{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZBSgV3zFoXF"
      },
      "source": [
        "# EE 508 HW 1 Part 2: Classification\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUucv9SGFoXG"
      },
      "source": [
        "## Cross Validation, Bias-Variance trade-off, Overfitting\n",
        "\n",
        "In this section, we will demonstrate data splitting and the validation process in machine learning paradigms. We will use the Iris dataset from the `sklearn` library.\n",
        "\n",
        "Objective:\n",
        "- Train a Fully-Connected Network (FCN) for classification.  \n",
        "- Partition the data using three-fold cross-validation and report the training, validation, and testing accuracy.  \n",
        "- Train the model using cross-entropy loss and evaluate it with 0/1 loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "evIZurwSFoXG"
      },
      "outputs": [],
      "source": [
        "# import required libraries and dataset\n",
        "import numpy as np\n",
        "# load sklearn for ML functions\n",
        "from sklearn.datasets import load_iris\n",
        "# load torch dataaset for training NNs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(['ggplot'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isq3OIR6FoXH"
      },
      "source": [
        "### **TODO 1**: Implement the cross validation function\n",
        "In this function, the dataset is first shuffled. Then, we need to implement a loop that iterates through each fold, selecting a subset of samples as the validation set while assigning the remaining samples to the training set, and stores these partitions in the `folds` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pgHxncT7FoXH"
      },
      "outputs": [],
      "source": [
        "def cross_validation(x: np.array, y: np.array, n_folds: int=3):\n",
        "    \"\"\"\n",
        "    Splitting the dataset to the given fold\n",
        "    Parameters:\n",
        "    - x: Feaures of the dataset, with shape (n_samples, n_features)\n",
        "    - y: Class label of the dataset, with shape (n_samples,)\n",
        "    - n_folds: the given number of partitions\n",
        "        For instnace, 5-fold CV with 100 percentage:\n",
        "        fold_1: training on 20~99, validation on 0~19(%)\n",
        "        fold_2: training on 0~19 and 40~99, validation on 20~39(%)\n",
        "        fold_3: training on 0~39 and 60~99, validation on 40~59(%)\n",
        "        fold_4: training on 0~59 and 80~99, validation on 60~79(%)\n",
        "        fold_5: training on 0~79, validation on 80~99(%)\n",
        "\n",
        "    Returns:\n",
        "    - folds (list): In the format with len(folds) == n_folds\n",
        "        [\n",
        "            (x_train_fold1, y_train_fold1, x_valid_fold1, y_valid_fold1),\n",
        "            (x_train_fold2, y_train_fold2, x_valid_fold2, y_valid_fold2),\n",
        "            (x_train_fold3, y_train_fold3, x_valid_fold3, y_valid_fold3),\n",
        "            ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "\n",
        "    folds = []\n",
        "    n_data = x.shape[0]\n",
        "    index = np.arange(n_data)\n",
        "    # shaffle the data with np.random.shuffle\n",
        "    np.random.shuffle(index)\n",
        "    # find the partition with numpy.linspace\n",
        "    #np.linspace(0, 100, num=6) â†’ [0, 20, 40, 60, 80, 100]\n",
        "    partitions = np.linspace(0, n_data, num=n_folds+1, endpoint=True) #equally spaced numbers from 0 to n_data\n",
        "\n",
        "    # Finish the code here\n",
        "    partitions = partitions.astype(int)\n",
        "\n",
        "    # Finish the code here\n",
        "    for i in range(n_folds):\n",
        "      validation_indices = index[partitions[i]:partitions[i+1]]\n",
        "      #np.setdiff1d(array1, array2) finds elements in array1 that are NOT in array2.\n",
        "      training_indices = np.setdiff1d(index, validation_indices)\n",
        "\n",
        "      valid_x = x[validation_indices]\n",
        "      valid_y = y[validation_indices]\n",
        "      train_x = x[training_indices]\n",
        "      train_y = y[training_indices]\n",
        "      folds.append((train_x, train_y, valid_x, valid_y))\n",
        "\n",
        "    print(f\"The Partitions:\")\n",
        "    for idx, (_, train_y, _, valid_y) in enumerate(folds):\n",
        "        print(f\"[Fold-{idx+1}] #Training: {train_y.shape[0]:4>0d}; #Validation: {valid_y.shape[0]:4>0d}\")\n",
        "        from collections import Counter\n",
        "        # you check check the label distribution\n",
        "        print(Counter(train_y)) # y is represented as train_y=[1,1,2,0,0,2,2,1] i.e class lables here so counter gives us count of each class\n",
        "        print(Counter(valid_y))\n",
        "\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7gGGdvkFoXH",
        "outputId": "03484794-7f54-4c97-a4b1-434ff5376e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Partitions:\n",
            "[Fold-1] #Training: 100; #Validation: 50\n",
            "Counter({1: 35, 2: 34, 0: 31})\n",
            "Counter({0: 19, 2: 16, 1: 15})\n",
            "[Fold-2] #Training: 100; #Validation: 50\n",
            "Counter({2: 35, 1: 33, 0: 32})\n",
            "Counter({0: 18, 1: 17, 2: 15})\n",
            "[Fold-3] #Training: 100; #Validation: 50\n",
            "Counter({0: 37, 1: 32, 2: 31})\n",
            "Counter({2: 19, 1: 18, 0: 13})\n"
          ]
        }
      ],
      "source": [
        "# fixed the random seed\n",
        "np.random.seed(42)\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "x, y = iris.data, iris.target\n",
        "# Split into training and testing sets\n",
        "three_folds = cross_validation(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape[1]) # shows number of features\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "8i0RRz1HShRT",
        "outputId": "4ccb0739-536d-43f6-8f56-4d1423220d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "(150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S-kza-6FoXI"
      },
      "source": [
        "### **TODO 2**: Build a Fully-Connect Networks with PyTorch\n",
        "In this section, we build simple FCN models with different numbers of hidden units for the classification task.\n",
        "\n",
        "- **Training:** Use cross-entropy for optimization.  \n",
        "- **Inferencing:** Evaluate with 0/1 loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zELyK8BYFoXI"
      },
      "outputs": [],
      "source": [
        "# define the FCN model\n",
        "class FCN_model(nn.Module):\n",
        "    # take the argument for the number of hidden units\n",
        "    def __init__(self, input_dim, output_dim=None, n_hidden=32):\n",
        "        # Finish the code here\n",
        "        super(FCN_model, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Finish the code here\n",
        "        #apply linear function then activation and then linear function\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5TTkatbFoXI"
      },
      "source": [
        "Set up the evaluation and training functions for the FCN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zTWwra6mFoXI"
      },
      "outputs": [],
      "source": [
        "def eval(model:nn.Module,\n",
        "         x:torch.tensor,\n",
        "         y:torch.tensor) -> float:\n",
        "    \"\"\"Evaluate the model: inference the model with 0/1 loss\n",
        "    We can define the output label is the maximum logit from the model\n",
        "\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x: input features\n",
        "    - y: ground truth labels, dtype=long\n",
        "\n",
        "    Returns:\n",
        "    - loss: the average 0/1 loss value\n",
        "    \"\"\"\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = torch.argmax(model(x), dim=1)\n",
        "\n",
        "    loss = 0\n",
        "    # Finish the code here\n",
        "    loss = (preds != y).sum().item()\n",
        "    print(f\"Averaging 0/1 loss: {loss/preds.shape[0]:.4f}\")\n",
        "    return loss/preds.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dsikDGD2FoXI"
      },
      "outputs": [],
      "source": [
        "def train(model:nn.Module,\n",
        "          x_train:torch.tensor,\n",
        "          y_train:torch.tensor,\n",
        "          x_valid:torch.tensor,\n",
        "          y_valid:torch.tensor,\n",
        "          epochs:int=300):\n",
        "    \"\"\"Trining process\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x_train, y_train: trainig features and labels (dtype=long)\n",
        "    - x_valid, y_valid: validation features and labels (dtype=long)\n",
        "    - epochs: number of the epoches for training\n",
        "    \"\"\"\n",
        "    # To simplify the process\n",
        "    # we do not take batches but use all the training samples\n",
        "    # set up the objective function and the optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        # Forward pass\n",
        "        outputs = model(x_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Cross Entropy Loss: {loss.item():.4f}\")\n",
        "            print(f\"[Train] \", end=\"\")\n",
        "            eval(model, x_train, y_train)\n",
        "            print(f\"[Valid] \", end=\"\")\n",
        "            eval(model, x_valid, y_valid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP0qfJOzFoXI"
      },
      "source": [
        "### **TODO 3**: Conduct the training/validation process in each fold\n",
        "We will use three-fold validation, meaning you need to train three models and report the training and validation loss for all three folds.  \n",
        "\n",
        "First, instantiate an FCN model with 32 hidden units.  \n",
        "Then, call the `train` function, which takes the training and validation folds created by the `cross_validation()` function, along with the model, as input. Set `epochs` to `500`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGatLGAkFoXI",
        "outputId": "125b5372-be5f-42cf-c4d1-b809745b68b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Traing Fold 0 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.6230\n",
            "[Train] Averaging 0/1 loss: 0.2800\n",
            "[Valid] Averaging 0/1 loss: 0.2000\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4807\n",
            "[Train] Averaging 0/1 loss: 0.0800\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [300/500], Cross Entropy Loss: 0.4038\n",
            "[Train] Averaging 0/1 loss: 0.0500\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [400/500], Cross Entropy Loss: 0.3473\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Epoch [500/500], Cross Entropy Loss: 0.3019\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0000\n",
            "===== Traing Fold 1 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.6566\n",
            "[Train] Averaging 0/1 loss: 0.3200\n",
            "[Valid] Averaging 0/1 loss: 0.3200\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4963\n",
            "[Train] Averaging 0/1 loss: 0.1500\n",
            "[Valid] Averaging 0/1 loss: 0.2400\n",
            "Epoch [300/500], Cross Entropy Loss: 0.4201\n",
            "[Train] Averaging 0/1 loss: 0.0800\n",
            "[Valid] Averaging 0/1 loss: 0.1200\n",
            "Epoch [400/500], Cross Entropy Loss: 0.3674\n",
            "[Train] Averaging 0/1 loss: 0.0500\n",
            "[Valid] Averaging 0/1 loss: 0.1000\n",
            "Epoch [500/500], Cross Entropy Loss: 0.3238\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Averaging 0/1 loss: 0.0300\n",
            "Averaging 0/1 loss: 0.0800\n",
            "===== Traing Fold 2 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.6086\n",
            "[Train] Averaging 0/1 loss: 0.1900\n",
            "[Valid] Averaging 0/1 loss: 0.2200\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4605\n",
            "[Train] Averaging 0/1 loss: 0.1000\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Epoch [300/500], Cross Entropy Loss: 0.3880\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [400/500], Cross Entropy Loss: 0.3376\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [500/500], Cross Entropy Loss: 0.2975\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0300\n",
            "Averaging 0/1 loss: 0.0200\n"
          ]
        }
      ],
      "source": [
        "train_losses, valid_losses = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Traing Fold {idx} =====\")\n",
        "    x_train = torch.Tensor(x_train)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    x_valid = torch.Tensor(x_valid)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "\n",
        "    # Finish the code here\n",
        "\n",
        "    num_classes = len(torch.unique(y_train))   # Count unique labels\n",
        "    model = FCN_model(input_dim=x_train.shape[1], n_hidden=32, output_dim=num_classes)\n",
        "    train(model, x_train, y_train, x_valid, y_valid, epochs=500)\n",
        "\n",
        "    train_losses.append(eval(model, x_train, y_train))\n",
        "    valid_losses.append(eval(model, x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fObp6F5PFoXJ",
        "outputId": "19d20171-f3c5-486c-dddf-62e67e17744c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.04,            0.00\n",
            "    1,          0.03,            0.08\n",
            "    2,          0.03,            0.02\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_losses, valid_losses)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1ybHVl_FoXJ"
      },
      "source": [
        "### **TODO4**: Check over-fitting with complex model\n",
        "We can follow the same procedure with a more complex FCN model.  \n",
        "Now, set the `number of hidden units` to `2048` and repeat the process for three-fold validation with `epochs = 500`.  \n",
        "The gap between the training and validation performance should increase.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b55fgDQGFoXJ",
        "outputId": "40babd00-1b71-408b-98e9-b68a012b2106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Traing Fold 0 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 1.1701\n",
            "[Train] Averaging 0/1 loss: 0.3400\n",
            "[Valid] Averaging 0/1 loss: 0.3200\n",
            "Epoch [200/500], Cross Entropy Loss: 0.0893\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [300/500], Cross Entropy Loss: 0.0846\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [400/500], Cross Entropy Loss: 0.0812\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0400\n",
            "Epoch [500/500], Cross Entropy Loss: 0.0786\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0400\n",
            "===== Traing Fold 1 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 1.3426\n",
            "[Train] Averaging 0/1 loss: 0.3400\n",
            "[Valid] Averaging 0/1 loss: 0.3400\n",
            "Epoch [200/500], Cross Entropy Loss: 0.0186\n",
            "[Train] Averaging 0/1 loss: 0.0100\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [300/500], Cross Entropy Loss: 0.0179\n",
            "[Train] Averaging 0/1 loss: 0.0100\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [400/500], Cross Entropy Loss: 0.0173\n",
            "[Train] Averaging 0/1 loss: 0.0100\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [500/500], Cross Entropy Loss: 0.0168\n",
            "[Train] Averaging 0/1 loss: 0.0100\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Averaging 0/1 loss: 0.0100\n",
            "Averaging 0/1 loss: 0.0600\n",
            "===== Traing Fold 2 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.5791\n",
            "[Train] Averaging 0/1 loss: 0.3200\n",
            "[Valid] Averaging 0/1 loss: 0.3600\n",
            "Epoch [200/500], Cross Entropy Loss: 0.0838\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Epoch [300/500], Cross Entropy Loss: 0.0794\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Epoch [400/500], Cross Entropy Loss: 0.0773\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Epoch [500/500], Cross Entropy Loss: 0.0758\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Averaging 0/1 loss: 0.0300\n",
            "Averaging 0/1 loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "train_overfit, valid_overfit = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Traing Fold {idx} =====\")\n",
        "    x_train = torch.Tensor(x_train)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    x_valid = torch.Tensor(x_valid)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "\n",
        "    # Finish the code here\n",
        "    num_classes = len(set(y_train.numpy()))  # Count unique labels\n",
        "    model = FCN_model(input_dim=x_train.shape[1], n_hidden=2048, output_dim=num_classes)\n",
        "    train(model, x_train, y_train, x_valid, y_valid, epochs=500)\n",
        "\n",
        "    train_overfit.append(eval(model, x_train, y_train))\n",
        "    valid_overfit.append(eval(model, x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-KRIajGBYXb6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9dK1E2fFoXJ",
        "outputId": "b361b31e-91e2-43ac-d974-4f41ce10c007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.02,            0.04\n",
            "    1,          0.01,            0.06\n",
            "    2,          0.03,            0.00\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_overfit, valid_overfit)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L28KrucFoXJ"
      },
      "source": [
        "### **TODO 5**: Compare the FCN with statistical ML models\n",
        "Here, we will use the Naive Bayes model from the `sklearn` library and perform three-fold validation.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9bFDsef7FoXJ"
      },
      "outputs": [],
      "source": [
        "# Load the Naive Bayes classifier from the library\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "train_nb, valid_nb = [], []\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "\n",
        "    # Finish the code here\n",
        "    model = GaussianNB()\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    train_acc = model.score(x_train, y_train)\n",
        "    valid_acc = model.score(x_valid, y_valid)\n",
        "\n",
        "    train_nb.append(1 - train_acc)\n",
        "    valid_nb.append(1 - valid_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SCBV8RWrFoXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6eaa33-0980-473f-bd32-ed009c72fe65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.05,            0.04\n",
            "    1,          0.02,            0.06\n",
            "    2,          0.04,            0.04\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_nb, valid_nb)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEEbCUD2FoXJ"
      },
      "source": [
        "### **TODO 6**:\n",
        "Answer the following questions in the next cell.  \n",
        "1. What is the the bias-variance trade-off in machine learning?\n",
        "2. How to reduce overfitting and underfitting?\n",
        "3. How do the training and inference processes differ between the Naive Bayes model and a fully connected neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt2Hcq4rFoXJ"
      },
      "source": [
        "1. Bias is an error that can cause the model to deviate from true value. If the model is too simple, errors would be large and is prone to underfitting. High Variance means that the model is very sensitive to training data and results in overly complex model which performs poorly on unseen data caufding overfitting. To strike a balance between these two, we need a trade off which is complex enough to capture the pattern well and also simple enough to generalise well.\n",
        "\n",
        "2.\n",
        "\n",
        "*  Overfitting can be reduced by\n",
        "  *   -Regularization : adding penality to large weights therby preventing over-complex models\n",
        "  *    -Cross validation - Ensures model is evaluated on different subsets of the data\n",
        "  *   Reducing complex layers.\n",
        "  *   Early Stopping : Prevents model from memorising the training data\n",
        "\n",
        "\n",
        "   *   \n",
        "      Underfitting can be avoided by -\n",
        "      \n",
        "  *   Adding more layers / neurons to the network\n",
        "\n",
        "  *  reducing regularisation\n",
        "  \n",
        "  *  Increase in the data quality and hyperparameter tuning\n",
        "  \n",
        "  *  Training for more epochs'\n",
        "  *   Adding more relavant features/ feature engineering\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "3. **Training:**\n",
        "\n",
        " Naive Bayes assumes that features are completely independent for a given class. It computes feature liklihood for each class using Maximum Likelihood estimation\n",
        "\n",
        "  Whereas, Fully connected Neural Network initialised weights and biases randomly, computes activations, predicts loss and computes gradients of loss wrt weights in backpropagation. This repeats until convergence\n",
        "\n",
        "  **Inference:**\n",
        "\n",
        "  In Naive Bayes posterior probabilities are computed and predictions are made with highest posteriror probability.\n",
        "\n",
        "  In fully connected model, activations are computed through all layers, final layer applied a softmax and chooses the class with highest probability\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PGpqYVM697KZ"
      },
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "caption",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}