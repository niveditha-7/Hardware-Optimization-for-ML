{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf7xy8JvRapf"
      },
      "source": [
        "# EE 508 HW 3 Part 1: Kernel Design\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip8ZTjLFEAnd"
      },
      "source": [
        "Install the `Ninja` package in Colab used for building PyTorch kernels and import all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o3kBpdYcS4R",
        "outputId": "dcaa4bce-02ce-4251-ea9a-87d0a54a9ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Ninja\n",
            "Successfully installed Ninja-1.11.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install Ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm2vufS4cRwB",
        "outputId": "dd5796da-0460-4b57-b84a-2f872383eb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVX15GjxRapg"
      },
      "source": [
        "## Im2col Algorithm\n",
        "\n",
        "\n",
        "`Im2col` is a method used in CNNs to transform the input data and filters into a format that allows the convolution to be expressed as a matrix multiplication. This transformation can simplify the implementation of convolution and leverage highly optimized matrix multiplication routines such as BLAS library.\n",
        "\n",
        "In the class and discussion, we have covered the `im2col` algorithm for 2D input with 2D filter. In this section, we extend and implement the `im2col` algorithm for 4D input with 4D filters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7d3Wm0aRaph"
      },
      "source": [
        "### **TODO 1**:\n",
        "\n",
        "Implement the `im2col` operation to transform the 4D input and filter tensors into two 2D matrices. After matrix multiplication, reshape the result back into a 4D tensor to simulate the convolution operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NK_g-XcncRwB"
      },
      "outputs": [],
      "source": [
        "def im2col_conv2d(input, filter, stride=1, padding=0):\n",
        "    \"\"\"\n",
        "    Perform a convolution operation using im2col transformation.\n",
        "\n",
        "    Parameters:\n",
        "    - input: Tensor of shape (N, C, H, W) -> Input image batch\n",
        "    - filter: Tensor of shape (K, C, KH, KW) -> Convolution filters\n",
        "    - stride: Stride of the convolution\n",
        "    - padding: Padding around the input\n",
        "\n",
        "    Returns:\n",
        "    - output_im2col: Tensor of shape (N, K, OH, OW) -> Convolution output\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract dimensions\n",
        "    N, C, H, W = input.shape\n",
        "    K, _, KH, KW = filter.shape\n",
        "\n",
        "    # Pad the input if necessary\n",
        "    if padding > 0:\n",
        "        input = F.pad(input, (padding, padding, padding, padding))\n",
        "\n",
        "    # Compute output height and width\n",
        "    OH = (H + 2 * padding - KH) // stride + 1\n",
        "    OW = (W + 2 * padding - KW) // stride + 1\n",
        "\n",
        "\n",
        "    # Initialize toeplitz tensor (im2col representation)\n",
        "    toeplitz = torch.zeros((N, C * KH * KW, OH * OW))\n",
        "\n",
        "\n",
        "    # Perform im2col transformation and store in toeplitz tensor\n",
        "    for i in range(OH):\n",
        "        for j in range(OW):\n",
        "            h_start = i * stride\n",
        "            h_end = h_start + KH\n",
        "            w_start = j * stride\n",
        "            w_end = w_start + KW\n",
        "\n",
        "\n",
        "            local_region = input[:, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "            toeplitz[:, :, i * OW + j] = local_region.reshape(N, -1)\n",
        "\n",
        "    # Reshape filter and toeplitz tensor for matrix multiplication\n",
        "    filter_reshaped = filter.view(K, -1)\n",
        "    toeplitz_reshaped = toeplitz.view(N, -1, OH * OW)\n",
        "\n",
        "\n",
        "    # Compute output by matrix multiplication\n",
        "    output = torch.matmul(filter_reshaped, toeplitz_reshaped)\n",
        "\n",
        "\n",
        "    # Reshape output\n",
        "    output_im2col = output.view(N, K, OH, OW)\n",
        "\n",
        "\n",
        "    return output_im2col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyQjlTxccRwB"
      },
      "source": [
        "Let's perform some tests. The results returned by `im2col_conv2d` function should match the results returned by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvlJ9lMecRwB",
        "outputId": "686726f0-efaf-4a69-e600-4a900fd39ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are matched: True\n"
          ]
        }
      ],
      "source": [
        "# Test case 1 with stride=1 and padding=0\n",
        "N, C, H, W = 4, 3, 5, 5  # Input dimensions\n",
        "K, _, KH, KW = 2, C, 3, 3  # Filter dimensions\n",
        "\n",
        "# Create random input and filter tensors\n",
        "torch.manual_seed(508)\n",
        "input_tensor = torch.randn(N, C, H, W)\n",
        "filter_tensor = torch.randn(K, C, KH, KW)\n",
        "\n",
        "# Perform convolution using im2col\n",
        "my_results = im2col_conv2d(input_tensor, filter_tensor, stride=1, padding=0)\n",
        "\n",
        "# Perform convolution using PyTorch\n",
        "pt_results = F.conv2d(input_tensor, filter_tensor, stride=1, padding=0)\n",
        "\n",
        "# Compare results\n",
        "print(\"Results are matched:\", torch.allclose(my_results, pt_results, atol=1e-6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKCWW8c8cRwC",
        "outputId": "f96ee3f9-0ec0-40bf-e44f-bd420778895d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are matched: True\n"
          ]
        }
      ],
      "source": [
        "# Test case 2 with stride=2 and padding=1\n",
        "# Perform convolution using im2col\n",
        "my_results = im2col_conv2d(input_tensor, filter_tensor, stride=2, padding=1)\n",
        "\n",
        "# Perform convolution using PyTorch\n",
        "pt_results = F.conv2d(input_tensor, filter_tensor, stride=2, padding=1)\n",
        "\n",
        "# Compare results\n",
        "print(\"Results are matched:\", torch.allclose(my_results, pt_results, atol=1e-6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6xZDfekcRwC"
      },
      "source": [
        "## PyTorch C++ Extension\n",
        "\n",
        "## Reordered Matrix Multiplication\n",
        "Matrix multiplication is one of the most critical operations in neural networks. While PyTorch provides a highly optimized `matmul` operator, re-implementing it from scratch can provide deeper insights into performance tuning. In this section, we will demonstrate how to implement a naive matrix multiplication (ijk ordered) kernel on CPU backend and integrate it with PyTorch.\n",
        "\n",
        "The code below demonstrates how to build PyTorch C++ extensions using just-in-time (JIT) compilation. The JIT compilation mechanism provides you with a way of compiling and loading your extensions on the fly using PyTorch's API `torch.utils.cpp_extension.load()` or `torch.utils.cpp_extension.load_inline()`.\n",
        "\n",
        "* `torch.utils.cpp_extension.load()` requires writing the C++ source code to a file and loading it from the filesystem.\n",
        "* `torch.utils.cpp_extension.load_inline()` functions similarly but takes the source code as a string rather than a file, which is the approach we will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QWi-vriJg942"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"\"\"\n",
        "torch::Tensor my_ijk_matmul(torch::Tensor a, torch::Tensor b) {\n",
        "    TORCH_CHECK(a.dim() == 2 && b.dim() == 2, \"Inputs must be 2D tensors.\");\n",
        "    TORCH_CHECK(a.dtype() == torch::kFloat32 && b.dtype() == torch::kFloat32, \"Inputs must be of type float32.\");\n",
        "    TORCH_CHECK(a.size(1) == b.size(0), \"Inner dimensions must match.\");\n",
        "\n",
        "    int m = a.size(0);\n",
        "    int n = a.size(1);\n",
        "    int p = b.size(1);\n",
        "\n",
        "    // Extract raw pointers to input tensors\n",
        "    const float* a_ptr = a.data_ptr<float>();\n",
        "    const float* b_ptr = b.data_ptr<float>();\n",
        "\n",
        "    // Define output tensor using torch::zeros (initialize with 0)\n",
        "    torch::Tensor output = torch::zeros({m, p}, a.options());\n",
        "    float* output_ptr = output.data_ptr<float>();\n",
        "\n",
        "    // Perform ijk-ordered matrix multiplication (output-stationary)\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < p; ++j) {\n",
        "            float sum = 0.0;  // Accumulator for C(i, j)\n",
        "            for (int k = 0; k < n; ++k) {\n",
        "                sum += a_ptr[i * n + k] * b_ptr[k * p + j];\n",
        "            }\n",
        "            output_ptr[i * p + j] = sum;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the extension\n",
        "my_kernel_lib = load_inline(\n",
        "    name=\"cpp_extension\",\n",
        "    cpp_sources=cpp_source,\n",
        "    functions=\"my_ijk_matmul\",\n",
        "    extra_cflags=['-O2'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDuErBr3cRwC"
      },
      "source": [
        "The kernel library has been loaded. Before testing it, we define some helper functions first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zxexARH_cRwC"
      },
      "outputs": [],
      "source": [
        "def check_correctness(func):\n",
        "    \"\"\"\n",
        "    Check correctness of the custom kernel.\n",
        "    \"\"\"\n",
        "    # Define input tensors\n",
        "    torch.manual_seed(508)\n",
        "    A = torch.randn(32, 32, dtype=torch.float32)\n",
        "    B = torch.randn(32, 32, dtype=torch.float32)\n",
        "\n",
        "    # Perform matrix multiplication using the custom kernel\n",
        "    my_results = func(A, B)\n",
        "\n",
        "    # Perform matrix multiplication using PyTorch\n",
        "    pt_results = torch.matmul(A, B)\n",
        "\n",
        "    # Compare results\n",
        "    if torch.allclose(my_results, pt_results, atol=1e-6):\n",
        "        print(\"Results match!\")\n",
        "    else:\n",
        "        print(\"Results do not match!\")\n",
        "        print(f\"my_results: {my_results}\")\n",
        "        print(f\"pt_results: {pt_results}\")\n",
        "\n",
        "\n",
        "def benchmark_matmul(func, num_runs=3, warmup_runs=3, size=1024):\n",
        "    \"\"\"\n",
        "    Benchmark a matrix multiplication function and compare it with PyTorch's matmul.\n",
        "\n",
        "    Parameters:\n",
        "    - func: The custom function to benchmark.\n",
        "    - num_runs: Number of timed executions for measurement.\n",
        "    - warmup_runs: Number of warm-up executions.\n",
        "    - size: Matrix dimension (size x size).\n",
        "\n",
        "    Returns:\n",
        "    - None (prints benchmark results)\n",
        "    \"\"\"\n",
        "\n",
        "    # Define input tensors\n",
        "    torch.manual_seed(508)\n",
        "    A = torch.randn(size, size, dtype=torch.float32)\n",
        "    B = torch.randn(size, size, dtype=torch.float32)\n",
        "\n",
        "    def measure_flops(kernel_func):\n",
        "        \"\"\"Helper function to measure FLOPs per second for a given function.\"\"\"\n",
        "\n",
        "        # Warm-up phase\n",
        "        for _ in range(warmup_runs):\n",
        "            kernel_func(A, B)\n",
        "\n",
        "        # Measure execution time over multiple runs\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_runs):\n",
        "            kernel_func(A, B)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Compute average time per run\n",
        "        avg_time = (end_time - start_time) / num_runs\n",
        "\n",
        "        # Estimate FLOPs: 2 * (m * n * p) for standard matrix multiplication\n",
        "        flops = 2 * size * size * size\n",
        "        flops_per_sec = flops / avg_time\n",
        "\n",
        "        return flops_per_sec, avg_time\n",
        "\n",
        "    # Benchmark the custom kernel\n",
        "    print(\"Benchmarking custom kernel...\")\n",
        "    flops_per_sec, avg_time = measure_flops(func)\n",
        "    print(f\"My kernel GFLOPs per second: {(flops_per_sec * 1e-9):.5f}, Average time: {avg_time:.5f} sec\")\n",
        "\n",
        "    # Benchmark PyTorch's matmul\n",
        "    print(\"\\nBenchmarking PyTorch matmul...\")\n",
        "    flops_per_sec, avg_time = measure_flops(torch.matmul)\n",
        "    print(f\"PyTorch GFLOPs per second: {(flops_per_sec * 1e-9):.5f}, Average time: {avg_time:.5f} sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAKo6KUvc0Gl"
      },
      "source": [
        "Let's compare the results returned by our customized kernel implmentation with the correct results returned by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jBpqPdOcRwC",
        "outputId": "4153cf8b-5b63-45a2-d5b0-df673a98df1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match!\n"
          ]
        }
      ],
      "source": [
        "check_correctness(my_kernel_lib.my_ijk_matmul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIcdNkyRc_lz"
      },
      "source": [
        "Benchmark the performance. This will take less one minute to finish, and you will find out that this implementation is about 25 times slower than PyTorch's built-in kernel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsMnHQC4cRwC",
        "outputId": "29e89ebc-0b7b-4ace-adf1-c46c9eeb29ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking custom kernel...\n",
            "My kernel GFLOPs per second: 0.52357, Average time: 4.10164 sec\n",
            "\n",
            "Benchmarking PyTorch matmul...\n",
            "PyTorch GFLOPs per second: 64.18508, Average time: 0.03346 sec\n"
          ]
        }
      ],
      "source": [
        "benchmark_matmul(my_kernel_lib.my_ijk_matmul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjvK7FSObdOG"
      },
      "source": [
        "### **TODO 2:**\n",
        "Implement **jki** ordered matmul kernel using the template below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nz_R-Nbfh_KN"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"\"\"\n",
        "torch::Tensor my_jki_matmul(torch::Tensor a, torch::Tensor b) {\n",
        "    TORCH_CHECK(a.dim() == 2 && b.dim() == 2, \"Inputs must be 2D tensors.\");\n",
        "    TORCH_CHECK(a.dtype() == torch::kFloat32 && b.dtype() == torch::kFloat32, \"Inputs must be of type float32.\");\n",
        "    TORCH_CHECK(a.size(1) == b.size(0), \"Inner dimensions must match.\");\n",
        "\n",
        "    int m = a.size(0);\n",
        "    int n = a.size(1);\n",
        "    int p = b.size(1);\n",
        "\n",
        "    // Extract raw pointers to input tensors\n",
        "    const float* a_ptr = a.data_ptr<float>();\n",
        "    const float* b_ptr = b.data_ptr<float>();\n",
        "\n",
        "    // Define output tensor using torch::zeros (initialize with 0)\n",
        "    torch::Tensor output = torch::zeros({m, p}, a.options());\n",
        "    float* output_ptr = output.data_ptr<float>();\n",
        "\n",
        "    // Perform jki-ordered matrix multiplication (B-stationary)\n",
        "    for (int j = 0; j < p; ++j) {          // Iterate over columns of B\n",
        "        for (int k = 0; k < n; ++k) {      // Iterate over common dimension\n",
        "            float b_val = b_ptr[k * p + j]; // Access B[k, j] (B is stationary)\n",
        "            for (int i = 0; i < m; ++i) {  // Iterate over rows of A\n",
        "                output_ptr[i * p + j] += a_ptr[i * n + k] * b_val; // Accumulate into C[i, j]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the extension\n",
        "my_kernel_lib = load_inline(\n",
        "    name=\"cpp_extension\",\n",
        "    cpp_sources=cpp_source,\n",
        "    functions=\"my_jki_matmul\",\n",
        "    extra_cflags=['-O2'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR2olXiGbTT4"
      },
      "source": [
        "We check correctness first and then benchmark it as previous example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEP5ahpKcS2c",
        "outputId": "0934c280-befa-4a2a-f683-7ea3ff2a4f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match!\n",
            "Benchmarking custom kernel...\n",
            "My kernel GFLOPs per second: 0.07123, Average time: 30.14785 sec\n",
            "\n",
            "Benchmarking PyTorch matmul...\n",
            "PyTorch GFLOPs per second: 62.77640, Average time: 0.03421 sec\n"
          ]
        }
      ],
      "source": [
        "check_correctness(my_kernel_lib.my_jki_matmul)\n",
        "benchmark_matmul(my_kernel_lib.my_jki_matmul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2odyFKd4bH_R"
      },
      "source": [
        "### **TODO 3:**\n",
        "Implement **ikj** ordered matmul kernel using the template below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "D0lRuUEecRwC"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"\"\"\n",
        "torch::Tensor my_ikj_matmul(torch::Tensor a, torch::Tensor b) {\n",
        "    TORCH_CHECK(a.dim() == 2 && b.dim() == 2, \"Inputs must be 2D tensors.\");\n",
        "    TORCH_CHECK(a.dtype() == torch::kFloat32 && b.dtype() == torch::kFloat32, \"Inputs must be of type float32.\");\n",
        "    TORCH_CHECK(a.size(1) == b.size(0), \"Inner dimensions must match.\");\n",
        "\n",
        "    int m = a.size(0);\n",
        "    int n = a.size(1);\n",
        "    int p = b.size(1);\n",
        "\n",
        "    // Extract raw pointers to input tensors\n",
        "    const float* a_ptr = a.data_ptr<float>();\n",
        "    const float* b_ptr = b.data_ptr<float>();\n",
        "\n",
        "    // Define output tensor using torch::zeros (initialize with 0)\n",
        "    torch::Tensor output = torch::zeros({m, p}, a.options());\n",
        "    float* output_ptr = output.data_ptr<float>();\n",
        "\n",
        "    // Perform ikj-ordered matrix multiplication (A-stationary)\n",
        "    for (int i = 0; i < m; ++i) {          // Iterate over rows of A\n",
        "        for (int k = 0; k < n; ++k) {      // Iterate over common dimension\n",
        "            float a_val = a_ptr[i * n + k]; // Access A[i, k] (A is stationary)\n",
        "            for (int j = 0; j < p; ++j) {  // Iterate over columns of B\n",
        "                output_ptr[i * p + j] += a_val * b_ptr[k * p + j]; // Accumulate into C[i, j]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the extension\n",
        "my_kernel_lib = load_inline(\n",
        "    name=\"cpp_extension\",\n",
        "    cpp_sources=cpp_source,\n",
        "    functions=\"my_ikj_matmul\",\n",
        "    extra_cflags=['-O2'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJl0LBMMhMqp",
        "outputId": "92d5002f-ab52-4bd0-975b-a8e937a08e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match!\n",
            "Benchmarking custom kernel...\n",
            "My kernel GFLOPs per second: 2.47943, Average time: 0.86612 sec\n",
            "\n",
            "Benchmarking PyTorch matmul...\n",
            "PyTorch GFLOPs per second: 65.14949, Average time: 0.03296 sec\n"
          ]
        }
      ],
      "source": [
        "check_correctness(my_kernel_lib.my_ikj_matmul)\n",
        "benchmark_matmul(my_kernel_lib.my_ikj_matmul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcYAhrAc9A6I"
      },
      "source": [
        "### **TODO 4:**\n",
        "Blocked matrix multiplication, also known as tiled matrix multiplication, can improve the temporal locality of inner loops. The general idea of blocking is to organize the data structures in a program into large chunks called blocks. (In this context, “block” refers to an application-level chunk of data, not to a cache block.) The program is structured so that it loads a chunk into the L1 cache, does all the reads and writes that it needs to on that chunk, then discards the chunk, loads in the next chunk, and so on.\n",
        "\n",
        "Implement **blocked ikj** ordered matmul kernel, with block size of 16, using the template below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MwczEMz69WPh"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"\"\"\n",
        "#define BLOCK_SIZE 16\n",
        "\n",
        "torch::Tensor my_blocked_ikj_matmul(torch::Tensor a, torch::Tensor b) {\n",
        "    TORCH_CHECK(a.dim() == 2 && b.dim() == 2, \"Inputs must be 2D tensors.\");\n",
        "    TORCH_CHECK(a.dtype() == torch::kFloat32 && b.dtype() == torch::kFloat32, \"Inputs must be of type float32.\");\n",
        "    TORCH_CHECK(a.size(1) == b.size(0), \"Inner dimensions must match.\");\n",
        "\n",
        "    int m = a.size(0);\n",
        "    int n = a.size(1);\n",
        "    int p = b.size(1);\n",
        "\n",
        "    // Extract raw pointers to input tensors\n",
        "    const float* a_ptr = a.data_ptr<float>();\n",
        "    const float* b_ptr = b.data_ptr<float>();\n",
        "\n",
        "    // Define output tensor using torch::zeros (initialize with 0)\n",
        "    torch::Tensor output = torch::zeros({m, p}, a.options());\n",
        "    float* output_ptr = output.data_ptr<float>();\n",
        "\n",
        "    // Blocked Matrix Multiplication\n",
        "    for (int ii = 0; ii < m; ii += BLOCK_SIZE) {\n",
        "        for (int kk = 0; kk < n; kk += BLOCK_SIZE) {\n",
        "            for (int jj = 0; jj < p; jj += BLOCK_SIZE) {\n",
        "\n",
        "                int i_end = std::min(ii + BLOCK_SIZE, m);\n",
        "                int k_end = std::min(kk + BLOCK_SIZE, n);\n",
        "                int j_end = std::min(jj + BLOCK_SIZE, p);\n",
        "\n",
        "\n",
        "                for (int i = ii; i < i_end; ++i) {\n",
        "                    for (int k = kk; k < k_end; ++k) {\n",
        "                        float a_val = a_ptr[i * n + k];\n",
        "                        for (int j = jj; j < j_end; ++j) {\n",
        "                            output_ptr[i * p + j] += a_val * b_ptr[k * p + j]; // Accumulate into C\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the extension\n",
        "my_kernel_lib = load_inline(\n",
        "    name=\"cpp_extension\",\n",
        "    cpp_sources=cpp_source,\n",
        "    functions=\"my_blocked_ikj_matmul\",\n",
        "    extra_cflags=['-O2'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mW9jbi59fMW",
        "outputId": "df373c49-22ca-465b-a23b-f7ccdf5cc04d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match!\n",
            "Benchmarking custom kernel...\n",
            "My kernel GFLOPs per second: 1.84913, Average time: 1.16135 sec\n",
            "\n",
            "Benchmarking PyTorch matmul...\n",
            "PyTorch GFLOPs per second: 39.75372, Average time: 0.05402 sec\n"
          ]
        }
      ],
      "source": [
        "check_correctness(my_kernel_lib.my_blocked_ikj_matmul)\n",
        "benchmark_matmul(my_kernel_lib.my_blocked_ikj_matmul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7GhQss-Rapi"
      },
      "source": [
        "## Data Parallel with SIMD\n",
        "\n",
        "Intel AVX (Advanced Vector Extensions) instructions are Single Instruction Multiple Data (SIMD) instructions that can process 8 single precision or 4 double precision floating-point operands in a single instruction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIUDCTQRapj"
      },
      "source": [
        "### **TODO 5:**\n",
        "\n",
        "Add Intel AVX SIMD (8xFP32) to implement **non-blocked ikj** ordered matmul using the template below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5m6ngTSjcRwD"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"\"\"\n",
        "#include <immintrin.h>  // AVX intrinsics\n",
        "\n",
        "torch::Tensor my_avx_ikj_matmul(torch::Tensor a, torch::Tensor b) {\n",
        "    TORCH_CHECK(a.dim() == 2 && b.dim() == 2, \"Inputs must be 2D tensors.\");\n",
        "    TORCH_CHECK(a.dtype() == torch::kFloat32 && b.dtype() == torch::kFloat32, \"Inputs must be of type float32.\");\n",
        "    TORCH_CHECK(a.size(1) == b.size(0), \"Inner dimensions must match.\");\n",
        "\n",
        "    int m = a.size(0);\n",
        "    int n = a.size(1);\n",
        "    int p = b.size(1);\n",
        "\n",
        "    // Extract raw pointers to input tensors\n",
        "    const float* a_ptr = a.data_ptr<float>();\n",
        "    const float* b_ptr = b.data_ptr<float>();\n",
        "\n",
        "    // Define output tensor using torch::zeros (initialize with 0)\n",
        "    torch::Tensor output = torch::zeros({m, p}, a.options());\n",
        "    float* output_ptr = output.data_ptr<float>();\n",
        "\n",
        "    // Perform ikj-ordered matrix multiplication with AVX\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int k = 0; k < n; ++k) {\n",
        "            __m256 a_vec = _mm256_set1_ps(a_ptr[i * n + k]);\n",
        "            for (int j = 0; j < p; j += 8) {\n",
        "                // Load 8 elements from B and C\n",
        "                __m256 b_vec = _mm256_loadu_ps(&b_ptr[k * p + j]);\n",
        "                __m256 c_vec = _mm256_loadu_ps(&output_ptr[i * p + j]);\n",
        "                c_vec = _mm256_fmadd_ps(a_vec, b_vec, c_vec);\n",
        "                _mm256_storeu_ps(&output_ptr[i * p + j], c_vec);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the extension\n",
        "my_kernel_lib = load_inline(\n",
        "    name=\"cpp_extension\",\n",
        "    cpp_sources=cpp_source,\n",
        "    functions=\"my_avx_ikj_matmul\",\n",
        "    extra_cflags=[\"-mavx\", \"-mfma\", \"-O2\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXvl9p8lcQWH",
        "outputId": "bc524295-4baf-4334-d243-b555758561aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match!\n",
            "Benchmarking custom kernel...\n",
            "My kernel GFLOPs per second: 7.90778, Average time: 0.27157 sec\n",
            "\n",
            "Benchmarking PyTorch matmul...\n",
            "PyTorch GFLOPs per second: 65.02407, Average time: 0.03303 sec\n"
          ]
        }
      ],
      "source": [
        "check_correctness(my_kernel_lib.my_avx_ikj_matmul)\n",
        "benchmark_matmul(my_kernel_lib.my_avx_ikj_matmul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Mt1ON8EAnj"
      },
      "source": [
        "## Multi-threading\n",
        "The `<thread>` library is a part of the C++11 standard library that provides classes and functions to manage threads. Using this library, you can create, manage, and synchronize threads directly from C++ code, which enables concurrent execution paths within your applications.\n",
        "\n",
        "### **TODO 6:**\n",
        "To further enhance the performance of the previous AVX implementation, employ multithreading by allocating two threads to execute the matrix multiplication concurrently. Each thread is assigned to process a distinct portion of matrix A: one thread handles the upper half, and the other handles the lower half."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FsaKBhZAEAnj"
      },
      "outputs": [],
      "source": [
        "cpp_source = \"\"\"\n",
        "#include <immintrin.h>  // AVX intrinsics\n",
        "#include <torch/extension.h>\n",
        "#include <vector>\n",
        "#include <thread>\n",
        "\n",
        "void thread_worker(const float* a_ptr, const float* b_ptr, float* output_ptr, int m_start, int m_end, int n, int p) {\n",
        "for (int i = m_start; i < m_end; ++i) {\n",
        "        for (int k = 0; k < n; ++k) {\n",
        "            __m256 a_vec = _mm256_set1_ps(a_ptr[i * n + k]);\n",
        "            for (int j = 0; j < p; j += 8) {\n",
        "                // Load 8 elements from B and C\n",
        "                __m256 b_vec = _mm256_loadu_ps(&b_ptr[k * p + j]);\n",
        "                __m256 c_vec = _mm256_loadu_ps(&output_ptr[i * p + j]);\n",
        "                c_vec = _mm256_fmadd_ps(a_vec, b_vec, c_vec);\n",
        "                _mm256_storeu_ps(&output_ptr[i * p + j], c_vec);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "torch::Tensor my_mt_avx_ikj_matmul(torch::Tensor a, torch::Tensor b) {\n",
        "    TORCH_CHECK(a.dim() == 2 && b.dim() == 2, \"Inputs must be 2D tensors.\");\n",
        "    TORCH_CHECK(a.dtype() == torch::kFloat32 && b.dtype() == torch::kFloat32, \"Inputs must be of type float32.\");\n",
        "    TORCH_CHECK(a.size(1) == b.size(0), \"Inner dimensions must match.\");\n",
        "\n",
        "    int m = a.size(0);\n",
        "    int n = a.size(1);\n",
        "    int p = b.size(1);\n",
        "\n",
        "    // Extract raw pointers to input tensors\n",
        "    const float* a_ptr = a.data_ptr<float>();\n",
        "    const float* b_ptr = b.data_ptr<float>();\n",
        "\n",
        "    // Define output tensor using torch::zeros (initialize with 0)\n",
        "    torch::Tensor output = torch::zeros({m, p}, a.options());\n",
        "    float* output_ptr = output.data_ptr<float>();\n",
        "\n",
        "    // Create two threads to parallelize over rows of A\n",
        "    int mid = m / 2;\n",
        "\n",
        "    std::thread t1(thread_worker, a_ptr, b_ptr, output_ptr, 0, mid, n, p);\n",
        "    std::thread t2(thread_worker, a_ptr, b_ptr, output_ptr, mid, m, n, p);\n",
        "\n",
        "    // Join threads\n",
        "    t1.join();\n",
        "    t2.join();\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load the extension\n",
        "my_kernel_lib = load_inline(\n",
        "    name=\"cpp_extension\",\n",
        "    cpp_sources=cpp_source,\n",
        "    functions=\"my_mt_avx_ikj_matmul\",\n",
        "    extra_cflags=[\"-mavx\", \"-mfma\", \"-O2\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jQbtgl49EAnj",
        "outputId": "ee41c959-4758-43c6-8020-7e9fc50007ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match!\n",
            "Benchmarking custom kernel...\n",
            "My kernel GFLOPs per second: 12.59500, Average time: 0.17050 sec\n",
            "\n",
            "Benchmarking PyTorch matmul...\n",
            "PyTorch GFLOPs per second: 59.78984, Average time: 0.03592 sec\n"
          ]
        }
      ],
      "source": [
        "check_correctness(my_kernel_lib.my_mt_avx_ikj_matmul)\n",
        "benchmark_matmul(my_kernel_lib.my_mt_avx_ikj_matmul)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "zollm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}